\section{Anforderungsanalyse}

Diese Arbeit verfolgt das Ziel, einen Vergleich der Leistung von React Native und Flutter durchzuführen. Der Vergleich wird durch verschiedene Benchmarks getätigt, welche eine konstante und vergleichbare Bewertungen der Frameworks ermöglichen. Es werden praxisnahe Anforderungen an die Frameworks gestellt, welche die Darstellung von Elementen und die Nutzungen der nativen Funktionen auf verschiedenen Endgeräten testen. Die fundamentale Forschungsfrage der Arbeit lautet:

\textit{Welche messbaren Unterschiede in der Darstellungsleistung ergeben sich im Vergleich zwischen Flutter und React Native in modernen Cross-Plattform-Anwendungen?}

Daraus lassen sich folgende Unterfragen ableiten:

\begin{enumerate}
    \item Wie groß ist der quantitative Unterschied der Frameworks bei der Darstellung von Elementen?
    \item Gibt es Unterschiede der Leistung auf verschiedenen Endgeräten?
    \item Welches Framework ist für die alltägliche Praxis im Hinblick auf die Leistung attraktiver?
\end{enumerate}

\subsection{Auswahl der Benchmarks}

Es wird ein Benchmark genutzt, welcher die Darstellung von verschiedenen Elementen mithilfe von Animationen beinhaltet. Es wurden Tests gewählt, welche die Darstellung von Listen, komplexeren Animationen und die Funktion von nativen Methoden überprüfen. Die Tests werden mit einem Chromium-basierten Browser und einem emulierten Android-Gerät durchgeführt. Zum Schluss werden die Messwerte jedes Benchmarks in einem Diagramm dargestellt, um eine optischen Vergleich der Resultate zu erhalten.

\subsubsection*{Würfel}

Es wird eine Anzahl an Würfeln auf dem Endgerät dargestellt. Diese Würfel sind in einem durchgehenden Farbwechsel und sie rotieren sich um ihre eigene Achse. Ebenfalls bewegen sie sich stetig zu zufälligen Positionen innerhalb des Bildschirmes. Es gibt zwei verschiedene Durchführungen, eine mit 100 dargestellten Würfeln und eine mit 1000 Würfeln. Es wird festgehalten, wie viele Frames pro Sekunde berechnet werden konnten. Die Messung erfolgt im Browser über den Chromium Perfomance Monitor innerhalb der DevTools. Dort lässt sich die Zeit ablesen, die es gebraucht hat einen einzelnen Frame darzustellen, wodurch sich die Frames pro Sekunde (FPS) ausrechnen lassen. Auf dem Android Gerät, gibt es einen eingebauten Perfomance Monitor, welcher mir direkt die FPS anzeigt.

\subsubsection*{Listen}

Bei diesem Benchmark wird simuliert, wie lange es dauert, bis eine große Liste mit vielen Eintrag vollständig angezeigt wird. Es gibt zwei verschiedene Durchführungen, eine mit einer Anzahl an Einträgen der Liste von 10000 und eine mit 100000. Die Daten der Liste werden von einem externen Package namens Faker generiert. Das Programm arbeitet in beiden Framworks gleich und berechnet die Daten mit Hilfe eines Alogrithmus, so dass die Vergleichbarkeit der Ergebnisse gewahrt bleibt. Der Zeitstempel wird zu Beginn der Darstellung gesetzt und die Zeitdifferenz wird mit dem Zeitstempel nach der Berechnung berechnet. Neben den Messwerten nach jedem Durchlauf, welche für die graphische Darstellung genutzt werden, wird der Median und der Mittelwert berechnet, damit man einen genaueren Vergleich der Daten erhält.

\subsubsection*{Bild-Upload}

Um die Effizienz der Frameworks in Bezug auf die nativen Funktionen der Endgeräte zu testen, wird das Hochladen eines Bildes vom Endgerät als Benchmark genutzt. Gemessen wird die Zeit, die nach dem Anklicken des Buttons zum Öffnen der Galerie vergeht, bis das Foto vollständig angezeigt wird. Es werden 30 verschiedene Bilder hochgeladen, damit Caching oder ähnliche Learning Faktoren beseitigt werden. Es werden bei beiden Frameworks die identischen Bilder hochgeladen und die einzelnen Messwerte nach jedem Hochladen, sowie der Median und der Mittelwert über die gesamte Durchführung, gewertet.

\subsection{Vergleichbarkeit der Benchmarks}

Die Benchmarks sind darauf ausgelegt, vergleichbare Messwerte zu produzieren. Es herrschen bei den beiden Cross-Platform Frameworks identische Bedingungen und die Umgebung ist identisch. Über einen zeitlichen Verlauf oder über die Anzahl der Durchführungen lassen sich die Messwerte eines einzelnen Durchlaufs chronologisch ordnen. Die Messungen werden insgesamt 3 mal durchgeführt, wodurch bei jeder Durchführung eine festgelegte Anzahl an Messwerten produziert wird. Es wird festgehalten, wie viele Bilder pro Sekunde dargestellt werden können oder wie lange es dauert, bis eine Darstellung fertig gerendert ist. 

